{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import and Initial Inspection**\n",
    "\n",
    "The dataset utilized in this project is derived from my Independent Research Project, featuring data sourced from the Canadian Department of Fisheries, the Bedford Institute of Oceanography, and GEBCO.The data represents a selected subset of variables of that will be used in the final project. \n",
    "\n",
    "This data has presence and absence locations for a species of Glass Sponge that has gloablly unique aggregations on the Nova Scotian Shelf. As deep sea surveys are few and far inbetween, modelling the potential habitat of the Glass Sponge is required in order to support responsible marine spatial planning and conservation efforts for Nova Scotias deep sea sponge ecosystems.\n",
    "\n",
    "The target variable 'Presence', encoded as a binary classification where Presence=1 signifies observed presence and Absence=2 indicates an observed absence. The explanatory variables comprise a diverse array of oceanogprahic factors intended to deliniate potential prefered habitats.\n",
    "\n",
    "This notebook serves as a guide to binary classification using a deep learning model called LSTM (Long Short-Term Memory) RNN networks. It shows initial data preprocessing steps such as column removal and data normalization, partitioning the dataset into training and testing, training the LSTM model, and evaluation of the model's ability to predict presence or absence based on oceanographic factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Latitude', 'Longitude', 'Presence', 'Slope', 'Depth',\n",
       "       'Bottom Current Mean', 'Surface Current Mean',\n",
       "       'Primary Productivity Mean', 'Bottom Salinity Mean',\n",
       "       'Surface Salinity Mean', 'Bottom Temp Mean', 'Bottom Temp Max',\n",
       "       'Bottom Temp Min', 'Surface Temp Mean', 'Surface Temp Min',\n",
       "       'Surface Temp Max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(r'C:\\NSCC\\winter\\DataMiningModelling\\DeepLearning\\GlassSponge_PA.csv')\n",
    "\n",
    "data.head()\n",
    "\n",
    "data.columns\n",
    "\n",
    "data= data.drop(['MISSION', 'YEAR', 'comm','OID_', 'SOURCE'], axis=1)\n",
    "\n",
    "data.rename(columns={\n",
    "    'Var_Slope': 'Slope',\n",
    "    'Var_Bathy': 'Depth',\n",
    "    'Var_Current_Bottom_Mean': 'Bottom Current Mean',\n",
    "    'Var_Current_Surface_Mean': 'Surface Current Mean',\n",
    "    'Var_pp_mean': 'Primary Productivity Mean',\n",
    "    'Var_Salinity_Bottom_Mean': 'Bottom Salinity Mean',\n",
    "    'Var_Salinity_Surface_Mean': 'Surface Salinity Mean',\n",
    "    'Var_Mean_Bottom_Temp': 'Bottom Temp Mean',\n",
    "    'Var_Mean_Surface_Temp': 'Surface Temp Mean',\n",
    "    'Var_Max_Bottom_Temp' : 'Bottom Temp Max',\n",
    "    'Var_Min_Bottom_Temp' : 'Bottom Temp Min',\n",
    "    'Var_Min_Surface_Temp': 'Surface Temp Min',\n",
    "    'Var_Max_Surface_Temp': 'Surface Temp Max', }, inplace=True)\n",
    "\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Cleaning**\n",
    "\n",
    "All rows with Null values are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values = data.isnull().sum()\n",
    "\n",
    "print(\"Number of null values in each column:\\n\", null_values)\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "print(\"New DataFrame shape after dropping rows with null values:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploring distributions of variables**\n",
    "\n",
    "First, I wanted to explore whether there were differences in the distribution of variables for my binary target, categorizing the data into 'Absence' and 'Presence' groups. This initial step was done to visualize how the variables' distribution differs across the two categories. \n",
    "\n",
    "I also plotted the overall distribution of each variable. This overview was aimed at determining the necessity for data preprocessing steps such as trimming outliers or applying transformations to improve the shape of the distributions. This highlighted lots of outliers and non-normal distributions that will be trimmed in later data preparation steps in order to optimize the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "for column in data.drop(['Presence', 'Latitude', 'Longitude'], axis=1).columns:\n",
    "    # Create a figure with 2 subplots side by side\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(20, 6)) \n",
    "\n",
    "    # Plot variables densities by presence column\n",
    "    sns.kdeplot(data[data['Presence'] == 0][column], label='Absent', fill=True, ax=axs[0])\n",
    "    sns.kdeplot(data[data['Presence'] == 1][column], label='Present', fill=True, ax=axs[0])\n",
    "    axs[0].set_title(f'Distribution of {column} by Presence')\n",
    "    axs[0].set_xlabel(column)\n",
    "    axs[0].set_ylabel('Density')\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Plot overall density of variables\n",
    "    sns.kdeplot(data[column], label='Overall', fill=True, ax=axs[1])\n",
    "    axs[1].set_title(f'Overall Distribution of {column}')\n",
    "    axs[1].set_xlabel(column)\n",
    "    axs[1].set_ylabel('Density')\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**\n",
    "\n",
    "Here I did preprocessing through outlier trimming, scaling, and transforming specific variables. It starts by removing outliers for the predictor values that lie beyond two standard deviations from the mean. Then the predictors are scaled using the RobustScaler, which reduces the influence of outliers. For negatively skewed variables, a Box-Cox transformation is applied to make their distribution more symmetric. Transformations were attempted on all variables, although only kept for the variables where the transfromation impoved the overal distribution. Finally, the code visualizes the original, scaled, and transformed distributions for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "\n",
    "np.random.seed(2010)\n",
    "\n",
    "# Standard deviation trimming\n",
    "num_std_dev = 2\n",
    "mean = data.drop(columns=['Presence', 'Latitude', 'Longitude']).mean()\n",
    "std_dev = data.drop(columns=['Presence', 'Latitude', 'Longitude']).std()\n",
    "outliers = ((data.drop(columns=['Presence', 'Latitude', 'Longitude']) - mean).abs() > num_std_dev * std_dev).any(axis=1)\n",
    "trimmed_data = data[~outliers]\n",
    "\n",
    "# After trimming, reset the index \n",
    "trimmed_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Scale the Predictors with robust scaler\n",
    "predictors = trimmed_data.drop(columns=['Presence', 'Latitude', 'Longitude'])\n",
    "target = trimmed_data['Presence']\n",
    "scaler = RobustScaler()\n",
    "scaled_predictors = scaler.fit_transform(predictors)\n",
    "scaled_data = pd.DataFrame(scaled_predictors, columns=predictors.columns)\n",
    "scaled_data['Presence'] = target\n",
    "\n",
    "df_transformed = scaled_data[['Presence','Surface Current Mean','Slope', 'Depth', 'Bottom Current Mean', \n",
    "                              'Primary Productivity Mean', 'Surface Salinity Mean', 'Surface Temp Mean', \n",
    "                              'Surface Temp Min', 'Surface Temp Max']].copy()\n",
    "\n",
    "negatively_skewed_variables = [ 'Bottom Salinity Mean', 'Bottom Temp Mean', 'Bottom Temp Max', 'Bottom Temp Min']\n",
    "\n",
    "# Apply Box-Cox transformation to negatively skewed variables\n",
    "for variable in negatively_skewed_variables:\n",
    "    data_positive = scaled_data[variable] - scaled_data[variable].min() + 1\n",
    "    transformed_data, _ = boxcox(data_positive)\n",
    "    df_transformed[variable] = transformed_data\n",
    "\n",
    "# Plot Distributions\n",
    "for column in df_transformed.drop(columns=['Presence']).columns:\n",
    "    if column in negatively_skewed_variables:\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(20, 6)) \n",
    "    else:\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(13, 6)) \n",
    "\n",
    "    # Original Distribution\n",
    "    sns.kdeplot(data[column], label='Original', fill=True, ax=axs[0])\n",
    "    axs[0].set_title(f'Original Distribution of {column}')\n",
    "    axs[0].set_xlabel(column)\n",
    "    axs[0].set_ylabel('Density')\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Scaled Distribution\n",
    "    sns.kdeplot(scaled_data[column], label='Scaled', fill=True, ax=axs[1])\n",
    "    axs[1].set_title(f'Trimmed and Scaled Distribution of {column}')\n",
    "    axs[1].set_xlabel(column)\n",
    "    axs[1].set_ylabel('Density')\n",
    "    axs[1].legend()\n",
    "\n",
    "    # Transformed Distribution (only for variables that were transformed)\n",
    "    if column in negatively_skewed_variables:\n",
    "        sns.kdeplot(df_transformed[column], label='Transformed', fill=True, ax=axs[2])\n",
    "        axs[2].set_title(f'Transformed Distribution of {column}')\n",
    "        axs[2].set_xlabel(column)\n",
    "        axs[2].set_ylabel('Density')\n",
    "        axs[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting to Tensors**\n",
    "\n",
    "This code selects predictors and the target variable from the dataset, excluding 'Presence' column from the predictors. Then predictors and target variables are converted into PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select predictors and target\n",
    "predictors = df_transformed.drop(['Presence'], axis=1).values\n",
    "target = df_transformed['Presence'].values\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.Tensor(predictors).unsqueeze(1) \n",
    "y = torch.Tensor(target)\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting Testing and Training**\n",
    "\n",
    "In this section of code the dataset is divided into training and testing with an 80-20 split, ensuring a balanced representation of both target classes.\n",
    "\n",
    " Originally, the code didn't have the `stratify=y` parameter, leading to a skewed distribution of the binary target y in the splits. This imbalance skewed the model towards predominantly predicting one class, resulting in a biased model. By introducing `stratify=y`, the distribution of the binary outcomes presence (1) and absence (0) is even across both training and testing sets. This adjustment significantly improved the models ability to generalize and accurately predict both classes. This little addition of code is the key for this models performance. Various runs where attempted before adding this parameter and all of them performed incredibly poorly with AOC values indicating 'worse than random'. None of the other preprocessing steps or additions to the code showed such a stark improvement as this one. Initially I didn't think including this was needed becuase the dataset was already evenly split between (1) and (0), although I learned that wasn't enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=2010)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Model**\n",
    "\n",
    "Defines a PyTorch model class for binary classification using an LSTM architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassificationLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1):\n",
    "        super(BinaryClassificationLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        lstm_out, (hn, _) = self.lstm(x)\n",
    "        lstm_out = lstm_out.contiguous().view(batch_size * seq_len, -1)\n",
    "        out = self.fc(lstm_out)\n",
    "        out = out.view(batch_size, seq_len, -1).squeeze(-1)\n",
    "        out = out[:, -1]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters**\n",
    "\n",
    "This code first sets the device to CUDA if available, otherwise defaults to CPU. Then initializes the LSTM model with defined input size, hidden layer size, and output size. It also defines Binary Cross Entropy with Logits as the loss function and Adam as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size = predictors.shape[1] \n",
    "output_size = 1\n",
    "hidden_size = 64  \n",
    "model = BinaryClassificationLSTM(input_size, hidden_size).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Model**\n",
    "\n",
    "This code trains the LSTM model for 100 epochs and calculates accuracy,loss, precison and recall for each epoch to monitor training progress.\n",
    "\n",
    "\n",
    "Using Root Mean Square Error (RMSE) for binary classification is possible but not ideal because RMSE focuses on the magnitude of prediction errors, which is more relevant to continuous data. In binary classification, where outcomes are categorical (0 or 1), RMSE doesn't reflect the model's classification accuracy or its ability to distinguish between the classes. Metrics like accuracy, precision, recall, F1, AUC-ROC and confusion matrix are more appropriate as they evaluate classification performance based on correct and incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)  \n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Calculate probabilities and binary predictions\n",
    "        train_outputs_probs = torch.sigmoid(model(X_train))\n",
    "        predictions = (train_outputs_probs >= 0.5).float()\n",
    "        \n",
    "        # Convert predictions and y_train to CPU numpy arrays necessary for sklearn metrics\n",
    "        predictions_np = predictions.cpu().numpy()\n",
    "        y_train_np = y_train.cpu().numpy()\n",
    "        \n",
    "        # Calculate accuracy, precision, recall, and F1 score\n",
    "        accuracy = accuracy_score(y_train_np, predictions_np)\n",
    "        precision = precision_score(y_train_np, predictions_np, zero_division=0)\n",
    "        recall = recall_score(y_train_np, predictions_np, zero_division=0)\n",
    "        f1 = f1_score(y_train_np, predictions_np, zero_division=0)\n",
    "        \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, '\n",
    "          f'Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Model on Testing Data**\n",
    "\n",
    "1. *Accuracy* measures the proportion of true results (both true positives and true negatives) among the total number of predictions. An accuracy of 94.80% means that the model's predictions are correct 95.8% of the time.\n",
    "\n",
    "2. *Precision* measures the proportion of true positive outcomes in the predictions made by your model. A precision of 94.07% means that the model predicted the positive class correctly 94.07% of the time. The results show a high precision, which indicates a low false positive rate.\n",
    "\n",
    "3. *Test Recall (Sensitivity)* measures the proportion of actual positives that were correctly identified by the model. A recall of 98.33% indicates that the model successfully identified nearly all true positives.\n",
    "\n",
    "4. *F1 Scores*  is the harmonic mean of precision and recall, providing a single metric to assess balance between them. An F1 score of 96.15% shows that the model achieves a balence between precision and recall. This is particularly useful when you care equally about false positives and false negatives, which in this case we do.\n",
    "\n",
    "*Overall Assessment of Model*\n",
    "\n",
    "This model demonstrates excellent performance across all the metrics (Accuracy, Precision, Recall, F1, AUC-ROC, Confusion Matrix). It has a high degree of accuracy in distinguishing between presence and absence, with a particularly high ability to identify true positives (high recall) and maintaining a low rate of false positives (high precision). The high AUC-ROC score indicates that the model's probability scores for the positive class was also good. \n",
    "\n",
    "\n",
    "It's suprising that this model, which is a type of Recurrent Neural Network (RNN) achieved such sucess at predicting this datasets binary predictions as this type of deep learning model is typically associated with time series analysis. This shows that RNNs are adeptive and can handle binary predictions and extract relevant patterns for a wide range of tasks, not limited to their conventional applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()  \n",
    "    test_outputs = torch.sigmoid(model(X_test))\n",
    "    \n",
    "    # Convert to binary predictions\n",
    "    predictions = (test_outputs >= 0.5).float()\n",
    "    \n",
    "    # Convert tensors to numpy arrays for use with sklearn metrics\n",
    "    predictions_np = predictions.cpu().numpy()\n",
    "    y_test_np = y_test.cpu().numpy()\n",
    "    \n",
    "    # Calculate accuracy, precision, recall, and F1 score\n",
    "    accuracy = accuracy_score(y_test_np, predictions_np)\n",
    "    precision = precision_score(y_test_np, predictions_np, zero_division=0)\n",
    "    recall = recall_score(y_test_np, predictions_np, zero_division=0)\n",
    "    f1 = f1_score(y_test_np, predictions_np, zero_division=0)\n",
    "    \n",
    "    # Print all metrics\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "    print(f'Test Precision: {precision:.4f}')\n",
    "    print(f'Test Recall: {recall:.4f}')\n",
    "    print(f'Test F1 Score: {f1:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Evaluation via AUC-ROC**\n",
    "\n",
    "*Area Under the Receiver Operating Characteristic (ROC) curve* is a performance measurement for classification problems such as binary predictions seen in this model. An AUC-ROC of 99.03% indicates the model is excellent at distinguishing between the positive and negative classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  \n",
    "    logits = model(X_test)\n",
    "    probabilities = torch.sigmoid(logits)\n",
    "    probabilities_np = probabilities.cpu().numpy()\n",
    "    y_test_np = y_test.cpu().numpy()\n",
    "    auc_roc = roc_auc_score(y_test_np, probabilities_np)\n",
    "    \n",
    "    print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Evaluation via Confusion Matrix**\n",
    "\n",
    "The confusion matrix gives breaks down the performance:\n",
    "True Negatives: 379\n",
    "False Positives: 52\n",
    "False Negatives: 14\n",
    "True Positives: 825\n",
    "This shows that the model has relatively few false positives and false negatives. It predicted postive incorrecrly 52 times and negative incorrectly 14 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = (probabilities >= 0.5).float()  # Convert probabilities to binary predictions\n",
    "conf_matrix = confusion_matrix(y_test.cpu().numpy(), y_pred.cpu().numpy())\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', cbar=False)\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.xticks(ticks=[0.5, 1.5], labels=['0', '1'])  \n",
    "plt.yticks(ticks=[0.5, 1.5], labels=['0', '1'], rotation=0) \n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
